{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:10.520657Z",
     "start_time": "2025-03-10T15:09:10.439760Z"
    }
   },
   "source": [
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import heapq\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:10.634464Z",
     "start_time": "2025-03-10T15:09:10.601825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###RADING FASTA###\n",
    "def read_fasta(filename):\n",
    "    \"\"\"\n",
    "    Read the first sequence from a FASTA file\n",
    "    Returns the sequence as a string (uppercase)\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    seq_lines =[line.strip() for line in lines if not line.startswith('>')]\n",
    "    genome_seq = \"\".join(seq_lines).upper()\n",
    "    return genome_seq\n"
   ],
   "id": "a84ee98a68c6bb1d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:10.767415Z",
     "start_time": "2025-03-10T15:09:10.693871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###GENERATING READS###\n",
    "def Generate_error_free_reads(genome_seq, N, l):\n",
    "    \"\"\"\n",
    "    in this function, given genome sequence we are generating N error-free reads of length l.\n",
    "    the reads are randomly sampled across the genome to achieve uniform coverage (on average)\n",
    "    :param genome_seq: string, the full PhiX genome\n",
    "    :param N: int, the number of reads\n",
    "    :param l: int, the length of each read\n",
    "    :return: list of read strings\n",
    "    \"\"\"\n",
    "    Genome_len = len(genome_seq)\n",
    "    circular_seq = genome_seq + genome_seq\n",
    "    reads =[]\n",
    "\n",
    "    for i in range(N):\n",
    "        start_position = random.randint(0, Genome_len - 1)\n",
    "        curr_read = circular_seq[start_position : start_position + l]\n",
    "        reads.append(curr_read)\n",
    "    return reads\n",
    "\n",
    "def mismatch_base(base):\n",
    "    \"\"\"\n",
    "    given a base return a different base\n",
    "    \"\"\"\n",
    "    bases =['A', 'C', 'G', 'T']\n",
    "    bases.remove(base)\n",
    "    return random.choice(bases)\n",
    "\n",
    "def generate_error_prone_reads(genome_seq, N, l, p):\n",
    "    \"\"\"\n",
    "    in this function, given genome sequence we are generating N error-prone reads of length l.\n",
    "    where each base is mutated with probability p.\n",
    "    :param genome_seq: string, the full PhiX genome\n",
    "    :param N: int, the number of reads\n",
    "    :param l: int, the length of each read\n",
    "    :param p: float, the probability of mutating each base\n",
    "    :return: list of read strings\n",
    "    \"\"\"\n",
    "    Genome_len = len(genome_seq)\n",
    "    circular_seq = genome_seq + genome_seq\n",
    "    reads = []\n",
    "    for i in range(N):\n",
    "        start_position = random.randint(0, Genome_len - 1)\n",
    "        curr_read_list = list(circular_seq[start_position: start_position + l])\n",
    "\n",
    "        for j in range(l):\n",
    "            if random.random() < p:\n",
    "                curr_read_list[j] = mismatch_base(curr_read_list[j])\n",
    "        reads.append(\"\".join(curr_read_list))\n",
    "    return reads\n",
    "\n",
    "def generate_reads(genome_seq, N, l, p=0.0):\n",
    "    \"\"\"\n",
    "    If p=0, return error-free reads.\n",
    "    Otherwise, return error-prone reads.\n",
    "    \"\"\"\n",
    "    if p == 0.0:\n",
    "        return Generate_error_free_reads(genome_seq, N, l)\n",
    "    else:\n",
    "        return generate_error_prone_reads(genome_seq, N, l, p)"
   ],
   "id": "dd95c315806b51de",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:11.002544Z",
     "start_time": "2025-03-10T15:09:10.976875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###K-MER INDEXING###\n",
    "def kmers_extraction(read,k):\n",
    "    \"\"\"\n",
    "    in this function we are extracting all k-mers from a given read\n",
    "    and returning a list of tuples (k-mer, starting_position)\n",
    "    \"\"\"\n",
    "    kmers =[]\n",
    "    for i in range(len(read)-k +1):\n",
    "        kmer=read[i:i+k]\n",
    "        kmers.append((kmer,i))\n",
    "    return kmers\n",
    "\n",
    "def kmer_index_build(reads,k):\n",
    "    \"\"\"\n",
    "    this function creates a dictionary that maps k-mers\n",
    "    to the reads that contains them, returns a dictionary\n",
    "    where keys are k-mers and values are set of reads indices.\n",
    "    \"\"\"\n",
    "    kmer_index={}\n",
    "    for i, read in enumerate(reads):\n",
    "        kmers = kmers_extraction(read,k)\n",
    "\n",
    "        for kmer,_ in kmers:\n",
    "            if kmer not in kmer_index:\n",
    "                kmer_index[kmer] = set()\n",
    "            kmer_index[kmer].add(i)\n",
    "    return kmer_index\n",
    "\n"
   ],
   "id": "b36ba31cd74f6312",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:11.115844Z",
     "start_time": "2025-03-10T15:09:11.075419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### one-pass EROOR CORRECTION###\n",
    "def build_kmer_frequency(reads, k):\n",
    "    \"\"\"\n",
    "    in this function we calculate how many times each k-mer appears in the entire set of\n",
    "    reads. we'll return a dictionary where the keys are k-mers and the values are their total counts\n",
    "    across all reads\n",
    "    \"\"\"\n",
    "    count = defaultdict(int)\n",
    "    for r in reads:\n",
    "        for i in range(len(r)-k+1):\n",
    "            kmer =r[i:i+k]\n",
    "            count[kmer] += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def naive_error_correction(reads, k=5, threshold=2):\n",
    "    \"\"\"\n",
    "    this is a single pass naive error correction:\n",
    "    first we build a global k-mer frequency across all reads,the for each read\n",
    "    for each k-mer region: if the frequency <threshold, single base changes is applied to find a higher\n",
    "    frequency k-mer, if a beneficial substitution found apply it\n",
    "    :return corrected reads\n",
    "    \"\"\"\n",
    "    frequency_map = build_kmer_frequency(reads, k)\n",
    "    corrected_reads = []\n",
    "\n",
    "    # iterate over each read\n",
    "    for org_read in reads:\n",
    "        read_list = list(org_read)\n",
    "        read_length = len(org_read)\n",
    "\n",
    "        #slide a window of length k across the read\n",
    "        for start_position in range(read_length - k + 1):\n",
    "            curr_kmer =org_read[start_position:start_position + k]\n",
    "            curr_freq = frequency_map[curr_kmer]\n",
    "\n",
    "            #consider substitutions only if current k-mer is below threshold\n",
    "            if curr_freq < threshold:\n",
    "                best_improvement = curr_freq\n",
    "                best_subtitution = None\n",
    "\n",
    "                #attempt single base substitutions whithin this k-mer\n",
    "                for offset in range(k):\n",
    "                    old_base = read_list[start_position + offset]\n",
    "                    for alternate_base in 'ACGT':\n",
    "                        if alternate_base == old_base:\n",
    "                            continue\n",
    "                        #temporarily substitute\n",
    "                        read_list[start_position + offset] = alternate_base\n",
    "\n",
    "                        #form a new k-mer and check frequency\n",
    "                        new_kmer = \"\".join(read_list[start_position:start_position+k])\n",
    "                        new_freq = frequency_map[new_kmer]\n",
    "                        if new_freq > best_improvement:\n",
    "                            best_improvement = new_freq\n",
    "                            best_subtitution = (start_position + offset, alternate_base)\n",
    "\n",
    "                    # Revert the single base substitution before moving on\n",
    "                    read_list[start_position + offset] = old_base\n",
    "\n",
    "                if best_subtitution is not None:\n",
    "                    index_in_read, new_base = best_subtitution\n",
    "                    read_list[index_in_read] = new_base\n",
    "        #re-assemble the corrected read\n",
    "        corrected_reads.append(\"\".join(read_list))\n",
    "\n",
    "    return corrected_reads"
   ],
   "id": "1eb1f0793938ce7e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:11.229911Z",
     "start_time": "2025-03-10T15:09:11.197930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###overlap###\n",
    "def overlap(read1, read2, min_overlap=1):\n",
    "    \"\"\"\n",
    "    given two reads, Return the length of the maximum overlap between\n",
    "    the suffix of read1 and the prefix of read2. If no overlap of at least min_overlap return 0\n",
    "    \"\"\"\n",
    "    maximum_len = min(len(read1), len(read2))\n",
    "    for length in range(maximum_len, min_overlap -1, -1):\n",
    "        if read1.endswith(read2[:length]):\n",
    "            return length\n",
    "    return 0\n",
    "\n",
    "def candidate_overlaps(reads,kmer_index,k):\n",
    "    \"\"\"\n",
    "    this function identifies pairs of reads that are likely to overlap\n",
    "    by checking for shared k-mers. so instead of comparing all reads pairwise\n",
    "    it quickly filters candidates.\n",
    "    :param reads: list of read strings\n",
    "    :param kmer_index: dictionary that maps k-mers to their index\n",
    "    :param k: k-mers length\n",
    "    :return: Dictionary where keys are read indices and values are\n",
    "             set of candidate overlapping reads\n",
    "    \"\"\"\n",
    "    candidates ={}\n",
    "\n",
    "    for i,read in enumerate(reads):\n",
    "        suffix =read[-k:]\n",
    "        prefix =read[:k]\n",
    "\n",
    "        for j in kmer_index.get(suffix,[]):\n",
    "            if i!=j:\n",
    "                if i not in candidates:\n",
    "                    candidates[i]=set()\n",
    "                candidates[i].add(j)\n",
    "\n",
    "        for j in kmer_index.get(prefix,[]):\n",
    "            if i!=j:\n",
    "                if j not in candidates:\n",
    "                    candidates[j]=set()\n",
    "                candidates[j].add(i)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def build_overlap_edges(reads, candidates, min_ovl=5):\n",
    "    \"\"\"\n",
    "    For each candidate pair, compute overlap length.\n",
    "    Return list of edges as [(-ovl_len, i, j)] for a max-heap.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for i, candidate_set in candidates.items():\n",
    "        for j in candidate_set:\n",
    "            overlap_len = overlap(reads[i], reads[j], min_ovl)\n",
    "            if overlap_len > 0:\n",
    "                # store as negative so a min heap can pop the largest overlap first\n",
    "                edges.append((-overlap_len, i, j))\n",
    "    return edges\n",
    "\n"
   ],
   "id": "f264643cc1dc8774",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:11.360791Z",
     "start_time": "2025-03-10T15:09:11.336413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def global_greedy_assemble(reads, edges, min_ovl=5):\n",
    "    \"\"\"\n",
    "    Use a max-heap of all edges, pick largest overlap, merge reads, update edges, repeat.\n",
    "    \"\"\"\n",
    "\n",
    "    active_indices = set(range(len(reads)))\n",
    "\n",
    "    heapq.heapify(edges)\n",
    "\n",
    "    while edges:\n",
    "        negative_ovl, i, j = heapq.heappop(edges)\n",
    "        overlap_len = -negative_ovl\n",
    "\n",
    "        if overlap_len < min_ovl:\n",
    "            break\n",
    "\n",
    "        #if either reads is no linger active,skip\n",
    "        if i not in active_indices or j not in active_indices:\n",
    "            continue\n",
    "\n",
    "        # Merge the two reads\n",
    "        merged_read = reads[i] + reads[j][overlap_len:]\n",
    "\n",
    "        #remove the old reads from active status\n",
    "        active_indices.remove(i)\n",
    "        active_indices.remove(j)\n",
    "\n",
    "        new_idx = len(reads)\n",
    "        reads.append(merged_read)\n",
    "        active_indices.add(new_idx)\n",
    "\n",
    "        # Now compute the new overlaps between this new read and all others\n",
    "        new_edges = []\n",
    "        for a in list(active_indices):\n",
    "            if a == new_idx:\n",
    "                continue\n",
    "\n",
    "            ov1 = overlap(reads[new_idx], reads[a], min_ovl)\n",
    "            if ov1 > 0:\n",
    "                new_edges.append((-ov1, new_idx, a))\n",
    "\n",
    "            ov2 = overlap(reads[a], reads[new_idx], min_ovl)\n",
    "            if ov2 > 0:\n",
    "                new_edges.append((-ov2, a, new_idx))\n",
    "\n",
    "        for edge in new_edges:\n",
    "            heapq.heappush(edges, edge)\n",
    "\n",
    "    final_contigs = [reads[idx] for idx in active_indices]\n",
    "    return final_contigs\n",
    "\n"
   ],
   "id": "ba3d688b2cfde5d5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:11.497091Z",
     "start_time": "2025-03-10T15:09:11.472531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###circular trimming###\n",
    "def trim_circular(contig, expected_length, min_search=50):\n",
    "    \"\"\"\n",
    "    Attempt to trim any duplicated region in a circular genome.\n",
    "    If 'contig' is longer than 'expected_length', we suspect\n",
    "    there's a repeated wrap-around region.\n",
    "    We search for the largest prefix that matches the suffix.\n",
    "    min_search is a minimum overlap size to look for.\n",
    "    Return a trimmed contig if duplication is found, else return original.\n",
    "    \"\"\"\n",
    "    #only trim contigs longer then the expected_length (~5386 bp)\n",
    "    if len(contig) <= expected_length:\n",
    "        return contig\n",
    "\n",
    "    excess_len = len(contig) - expected_length\n",
    "    max_check = min(len(contig)//2, excess_len + 200)\n",
    "\n",
    "    for check in range(max_check, min_search-1, -1):\n",
    "        prefix_sub = contig[:check]\n",
    "        suffix_sub = contig[-check:]\n",
    "        if prefix_sub == suffix_sub:\n",
    "            return contig[:-check]\n",
    "    return contig"
   ],
   "id": "7e53fa12122f13de",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:11.681591Z",
     "start_time": "2025-03-10T15:09:11.659953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_coverage(N, l, genome_len):\n",
    "    \"\"\"\n",
    "    Compute average coverage: (N * l) / genome_length\n",
    "    \"\"\"\n",
    "    return (N * l) / float(genome_len)\n"
   ],
   "id": "47721e8329862028",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:11.980474Z",
     "start_time": "2025-03-10T15:09:11.948977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def basic_performance_metrics(contigs, reference):\n",
    "    num_contigs = len(contigs)\n",
    "    lengths = [len(c) for c in contigs]\n",
    "    longest = max(lengths) if lengths else 0\n",
    "    total_assembled_length = sum(lengths)\n",
    "    return {\n",
    "        \"num_contigs\": num_contigs,\n",
    "        \"longest_contig\": longest,\n",
    "        \"total_assembled_length\": total_assembled_length,\n",
    "        \"reference_length\": len(reference),\n",
    "    }"
   ],
   "id": "8be5affa8f10bbef",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:09:12.075104Z",
     "start_time": "2025-03-10T15:09:12.042223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(fasta_file,\n",
    "         N_values = [250, 500, 1000],\n",
    "         l_values = [50, 100, 150],\n",
    "         p_values = [0.0, 0.01],\n",
    "         min_ovl =5):\n",
    "    reference_genome = read_fasta(fasta_file)\n",
    "    G = len(reference_genome)\n",
    "    print(f\"Reference genome length: {G}\")\n",
    "\n",
    "    print(\"\\n=== K-mer Filter,Error Correction, Global Greedy Merge + Circular Trim ===\")\n",
    "    print(f\"{'N':>6} {'l':>6} {'p':>6} {'Coverage':>8} {'#Contigs':>8} {'Longest':>8} \"\n",
    "      f\"{'TotalAsm':>9} {'TimeBuild':>6} {'TimeAssemble':>8}\")\n",
    "\n",
    "    for (N, l, p) in itertools.product(N_values, l_values, p_values):\n",
    "        coverage = compute_coverage(N, l, G)\n",
    "\n",
    "        # 1) Generate reads\n",
    "        t0 = time.time()\n",
    "        reads = generate_reads(reference_genome, N, l, p)\n",
    "\n",
    "        # 2) Optional error correction if p>0\n",
    "        if p > 0.0:\n",
    "            reads = naive_error_correction(reads, k=5, threshold=2)\n",
    "        gen_time = time.time() - t0\n",
    "\n",
    "        # 3) Build k-mer index & find candidate pairs\n",
    "        t0 = time.time()\n",
    "        k_size = max(5, l // 3)\n",
    "        k_idx = kmer_index_build(reads, k_size)\n",
    "        cands = candidate_overlaps(reads, k_idx, k_size)\n",
    "        edges = build_overlap_edges(reads, cands, min_ovl)\n",
    "        build_time = time.time() - t0\n",
    "\n",
    "        # 4) Global Greedy assembly\n",
    "        t0 = time.time()\n",
    "        contigs = global_greedy_assemble(reads, edges, min_ovl)\n",
    "        asm_time = time.time() - t0\n",
    "\n",
    "        # 5) Circular trimming\n",
    "        trimmed_contigs = []\n",
    "        for c in contigs:\n",
    "            trimmed_c = trim_circular(c, G, min_search=50)\n",
    "            trimmed_contigs.append(trimmed_c)\n",
    "\n",
    "        # 6) Performance\n",
    "        metrics = basic_performance_metrics(trimmed_contigs, reference_genome)\n",
    "        num_contigs = metrics[\"num_contigs\"]\n",
    "        longest_contig = metrics[\"longest_contig\"]\n",
    "        total_asm_len = metrics[\"total_assembled_length\"]\n",
    "\n",
    "        print(f\"{N:>6} {l:>6} {p:>6} {coverage:>8.2f} {num_contigs:>8} {longest_contig:>8} \"\n",
    "          f\"{total_asm_len:>9} {build_time:>6.2f} {asm_time:>8.2f}\")\n"
   ],
   "id": "d1cac7908d34e1cc",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:16:21.775557Z",
     "start_time": "2025-03-10T15:09:12.102883Z"
    }
   },
   "cell_type": "code",
   "source": "main(\"sequence.fasta\")",
   "id": "f935187df295dbcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference genome length: 5386\n",
      "\n",
      "=== K-mer Filter,Error Correction, Global Greedy Merge + Circular Trim ===\n",
      "     N      l      p Coverage #Contigs  Longest  TotalAsm TimeBuild TimeAssemble\n",
      "   250     50    0.0     2.32       25      501      5073   0.03     2.82\n",
      "   250     50   0.01     2.32      101      326      8419   0.04     1.68\n",
      "   250    100    0.0     4.64        5     2871      5268   0.10     2.67\n",
      "   250    100   0.01     4.64       94      580     16818   0.03     2.34\n",
      "   250    150    0.0     6.96        1     5402      5402   0.03     6.95\n",
      "   250    150   0.01     6.96      118      925     29196   0.11     5.24\n",
      "   500     50    0.0     4.64        4     2068      5346   0.06     9.23\n",
      "   500     50   0.01     4.64      124      390     12045   0.13     6.83\n",
      "   500    100    0.0     9.28        1     5398      5398   1.03    13.76\n",
      "   500    100   0.01     9.28      168      819     31021   0.07     9.63\n",
      "   500    150    0.0    13.92        1     5386      5386   0.19    18.04\n",
      "   500    150   0.01    13.92      185      997     53052   0.28    20.64\n",
      "  1000     50    0.0     9.28        1     5400      5400   0.17    16.97\n",
      "  1000     50   0.01     9.28      182      558     20424   0.13    28.54\n",
      "  1000    100    0.0    18.57        1     5386      5386   0.69    54.00\n",
      "  1000    100   0.01    18.57      201     1316     53972   0.32    57.68\n",
      "  1000    150    0.0    27.85        1     5386      5386   0.41    75.75\n",
      "  1000    150   0.01    27.85      197     2338     97196   0.81    91.56\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:16:21.822526Z",
     "start_time": "2025-03-10T15:16:21.791262Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "afa9c01e448b8dd",
   "outputs": [],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
